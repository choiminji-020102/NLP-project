{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz5fOfpcxMVnAccFwFF0R0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choiminji-020102/NLP_project/blob/main/%EC%8B%A4%EC%8A%B504_230920.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention"
      ],
      "metadata": {
        "id": "ajeU3FbPdsxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OE0MbupdgSn"
      },
      "outputs": [],
      "source": [
        "### 필요한 라이브러리 임폴트\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random  # 랜덤시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### scaled dot product attention 함수 정의\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v):\n",
        "  attention_score = tf.linalg.matmul(a=q, b=k, transpose_b=True)  # 행렬곱\n",
        "  print(f'attention score : \\n{attention_score}')\n",
        "  print('-'*80)\n",
        "\n",
        "  # Scaling\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # dk = 512.0\n",
        "  scaled_attention_score = attention_score / tf.math.sqrt(dk)\n",
        "  print(f'scale attention score : \\n{scaled_attention_score}')\n",
        "  print('-'*80)\n",
        "\n",
        "  # softmax 함수 실행\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_score, axis=-1)\n",
        "\n",
        "  # attention output 생성 --> attention_weights * value\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "metadata": {
        "id": "h9DOUnIYfqmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 실습용 데이터 생성\n",
        "\n",
        "'''\n",
        "1. sentence = \"I love you\"\n",
        "2. 토큰화 --> [I, love, you]\n",
        "3. 각 토큰 --> 512 차원의 임베딩 벡터로 변환  --> (3, 512) 임베딩 행렬 생성\n",
        "4. 표준 정규 분포로부터 랜덤한 실수 샘플링 --> (3, 512) 임베딩 행렬에 해당하는 데이터 생성\n",
        "'''\n",
        "\n",
        "# 랜덤 시드 설정\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# (3, 512) 임베딩 행렬에 해당하는 데이터 생성\n",
        "embedding_shape = (3, 512)\n",
        "embedding_metrix = tf.random.normal(shape=embedding_shape, mean=0, stddev=1, seed=0)  # 정규분포로부터 랜덤한 값을 산출, 매개변수로 평균, 표준편차 지정-> 표준정규분포\n",
        "\n",
        "# 결과 확인하기\n",
        "print(f'각 단어에 대한 임베딩 행렬 생성의 결과 : \\n{embedding_metrix}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN5l_iy2Rf1p",
        "outputId": "5e4bb68c-7622-4d9d-941c-a70183320bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "각 단어에 대한 임베딩 행렬 생성의 결과 : \n",
            "[[ 0.12391895  2.798774   -1.7961729  ... -1.2059427   0.26270926\n",
            "   0.7571296 ]\n",
            " [ 1.1822435   0.14100952 -1.9312251  ...  0.13743407 -0.35198182\n",
            "  -0.2877966 ]\n",
            " [ 0.5543462  -0.9617468  -0.59118867 ...  0.8685468   0.2526299\n",
            "  -1.1544628 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 가중치 행렬 W_q, W_k, W_v 생성\n",
        "'''\n",
        "1. 가중치 행렬 W_q, W_k, W_v의 모양 : (512, 512)\n",
        "2. 가중치 행렬의 초기 값 --> 랜덤한 실수로 설정\n",
        "3. 표준 정규 분포로부터 랜덤한 실수 샘플링 --> (512, 512) 가중치 행렬에 해당하는 데이터 W_q, W_k, W_v 생성\n",
        "'''\n",
        "\n",
        "# (512, 512*3) 모양에 해당하는 가중치 행렬 데이터 생성\n",
        "weights_shape = (512, 512*3)\n",
        "weights_matrix = tf.random.normal(shape=weights_shape, mean=0, stddev=1, seed=0)\n",
        "\n",
        "# 전체 가중치 행렬의 생성 결과 확인하기\n",
        "print(f'생성된 가중치 행렬 : \\n{weights_matrix}')\n",
        "print('-'*80)\n",
        "print(f'생성된 가중치 행렬의 모양 : {weights_matrix.shape}')\n",
        "\n",
        "print('-'*80)\n",
        "\n",
        "# W_q, W_k, W_v 생성 결과 확인하기\n",
        "W_q = weights_matrix[:, 0:512]        # 0~511\n",
        "W_k = weights_matrix[:, 512:512*2]    # 512~1023\n",
        "W_v = weights_matrix[:, 512*2:512*3]  # 1024~1535\n",
        "print(f'가중치 행렬 W_q의 모양 : {W_q.shape}')\n",
        "print('-'*80)\n",
        "print(f'가중치 행렬 W_k의 모양 : {W_k.shape}')\n",
        "print('-'*80)\n",
        "print(f'가중치 행렬 W_v의 모양 : {W_v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYwb9OI6T9C-",
        "outputId": "60c12191-16d3-4543-a9c5-1cc7dccc9da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 가중치 행렬의 모양 : (512, 1536)\n",
            "생성된 가중치 행렬의 모양 : [[ 0.71533036  1.6461307   1.3800917  ...  1.3516908  -0.93132895\n",
            "   0.7388974 ]\n",
            " [-0.27234635 -0.79476106 -1.7289692  ...  0.8388679  -0.25359774\n",
            "   0.14112103]\n",
            " [-0.5283124  -0.08258526  0.73241115 ... -1.0264676   0.8124797\n",
            "  -0.1822157 ]\n",
            " ...\n",
            " [ 0.6553392  -0.38194245 -0.6747198  ... -0.3752203  -0.04806583\n",
            "  -1.8004427 ]\n",
            " [-1.0859361   1.2909616  -0.07064369 ... -0.12155849  0.08048517\n",
            "  -0.66522104]\n",
            " [ 0.92764616  1.896207   -0.2973476  ...  0.8254814   0.07658196\n",
            "   1.2026174 ]]\n",
            "--------------------------------------------------------------------------------\n",
            "가중치 행렬 W_q의 모양 : (512, 512)\n",
            "--------------------------------------------------------------------------------\n",
            "가중치 행렬 W_k의 모양 : (512, 512)\n",
            "--------------------------------------------------------------------------------\n",
            "가중치 행렬 W_v의 모양 : (512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### q, k, v 생성\n",
        "\n",
        "'''\n",
        "임베딩 행렬과 가중치 행렬곱 --> 각 단어의 k, q, v 벡터 생성\n",
        "'''\n",
        "\n",
        "# 전체 단어의 qkv 행렬\n",
        "qkv = tf.linalg.matmul(a=embedding_matrix, b=weights_matrix)\n",
        "\n",
        "# 각 단어의 q, k, v 행렬 분해\n",
        "q = qkv[:, 0:512]\n",
        "k = qkv[:, 512:512*2]\n",
        "v = qkv[:, 512*2:512*3]\n",
        "\n",
        "# 결과 확인\n",
        "print(f'생성된 qkv의 모양 : {qkv.shape}')\n",
        "print(f'생성된 qkv의 값 : \\n{qkv}')\n",
        "print('-'*80)\n",
        "print(f'생성된 q 행렬의 모양 : {q.shape}')\n",
        "print(f'생성된 q 행렬의 값 : \\n{q}')\n",
        "print('-'*80)\n",
        "print(f'생성된 k 행렬의 모양 : {k.shape}')\n",
        "print(f'생성된 k 행렬의 값 : \\n{k}')\n",
        "print('-'*80)\n",
        "print(f'생성된 v 행렬의 모양 : {v.shape}')\n",
        "print(f'생성된 v 행렬의 값 : \\n{v}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD8Nfm2bV9qJ",
        "outputId": "586451aa-1ff9-463c-e528-848c761d0adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 qkv의 모양 : (3, 1536)\n",
            "생성된 qkv의 값 : \n",
            "[[ -4.5343094 -12.575607  -10.582888  ...  27.454405    8.589026\n",
            "    3.7817764]\n",
            " [ 20.533495    9.762379  -12.754654  ...  -7.790348    3.0119133\n",
            "  -22.428392 ]\n",
            " [-13.109122  -23.546644   27.74455   ...  10.989037   -9.120724\n",
            "    5.5862446]]\n",
            "--------------------------------------------------------------------------------\n",
            "생성된 q 행렬의 모양 : (3, 512)\n",
            "생성된 q 행렬의 값 : \n",
            "[[ -4.5343094 -12.575607  -10.582888  ...   1.725666    9.89917\n",
            "   53.45435  ]\n",
            " [ 20.533495    9.762379  -12.754654  ...  22.16268   -14.338249\n",
            "  -26.342205 ]\n",
            " [-13.109122  -23.546644   27.74455   ...  10.530284  -24.360441\n",
            "  -19.25621  ]]\n",
            "--------------------------------------------------------------------------------\n",
            "생성된 k 행렬의 모양 : (3, 512)\n",
            "생성된 k 행렬의 값 : \n",
            "[[-18.76557    41.255527   48.42447   ...  41.221973  -16.306606\n",
            "  -37.625732 ]\n",
            " [  7.694971    5.5027666  19.098648  ...  44.143616   -7.12607\n",
            "    5.886017 ]\n",
            " [ 26.271389   -1.7482119 -36.467995  ...  15.689399  -13.981336\n",
            "   -3.7377396]]\n",
            "--------------------------------------------------------------------------------\n",
            "생성된 v 행렬의 모양 : (3, 512)\n",
            "생성된 v 행렬의 값 : \n",
            "[[-33.561104   23.670322   18.042593  ...  27.454405    8.589026\n",
            "    3.7817764]\n",
            " [ 13.003802    6.162897   14.793936  ...  -7.790348    3.0119133\n",
            "  -22.428392 ]\n",
            " [-26.488708   12.374304  -12.5068245 ...  10.989037   -9.120724\n",
            "    5.5862446]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### scaled_dot_product_attention 함수 실행\n",
        "output, attention_weights = scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "# 결과 확인하기\n",
        "print(f'attention_weights : \\n{attention_weights}')\n",
        "print('-'*80)\n",
        "print(f'attention_weights 모양 : {attention_weights.shape}')\n",
        "print('-'*80)\n",
        "print(f'각 단어 별 새로 생성된 임베딩 벡터 : \\n{output}')\n",
        "print('-'*80)\n",
        "print(f'각 단어 별 새로 생성된 임베딩 벡터의 모양 : {output.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9493nu0FeC4V",
        "outputId": "5ce99555-70d4-41f5-dbb4-fd4adf2edf3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention score : \n",
            "[[16822.832  -1244.0242  -909.0869]\n",
            " [ 1226.6904 23283.93   -5462.8623]\n",
            " [ 6825.705   6555.3384 12889.601 ]]\n",
            "--------------------------------------------------------------------------------\n",
            "scale attention score : \n",
            "[[ 743.4712    -54.978622  -40.176346]\n",
            " [  54.21257  1029.014    -241.4267  ]\n",
            " [ 301.6564    289.70776   569.64526 ]]\n",
            "--------------------------------------------------------------------------------\n",
            "attention_weights : \n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "--------------------------------------------------------------------------------\n",
            "attention_weights 모양 : (3, 3)\n",
            "--------------------------------------------------------------------------------\n",
            "각 단어 별 새로 생성된 임베딩 벡터 : \n",
            "[[ -7.184684   -7.363205  -10.606537  ... -11.005928  104.29862\n",
            "   11.29334  ]\n",
            " [ 37.883698  -18.500378   27.66071   ...  25.135515   12.236287\n",
            "   27.44082  ]\n",
            " [ -7.512265   33.436974    7.411913  ...  35.68959    -1.2721348\n",
            "   16.808563 ]]\n",
            "--------------------------------------------------------------------------------\n",
            "각 단어 별 새로 생성된 임베딩 벡터의 모양 : (3, 512)\n"
          ]
        }
      ]
    }
  ]
}